# ğŸš€ AllianceCoder: What to Retrieve for Effective Retrieval-Augmented Code Generation?   ğŸ” An Empirical Study and Beyond  

## ğŸŒŸ Overview  

ğŸ–¥ï¸ **AllianceCoder** consists of three key phases:  

![Proposed_methodology.png](pics/AllianceCoder.png)  

### ğŸ“Œ 1. Repository API Processing  

âœ… We utilize **large language models (LLMs)** to generate **natural language descriptions** for each API present in the repository.  
âœ… These descriptions are then **encoded into vector representations** using pre-trained embedding models.  

### ğŸ“Œ 2. Query Processing  

ğŸ” We guide LLMs with **carefully designed examples** to generate **descriptions of potentially invoked API functionalities**.  
ğŸ” These descriptions are similarly **encoded into vector representations**.  

### ğŸ“Œ 3. Context-Integrated Code Generation 

ğŸ¤– **Relevant APIs are retrieved** based on the **cosine similarity** between their **vector representations**.  
ğŸ¤– The retrieved APIs provide **valuable context** for **enhanced code generation**.  

## ğŸ“‚ Project Structure  

```md
|-- ğŸ“ repo_funcs_summary   # ğŸ› ï¸ First step: Extract and summarize repository functions
|
|-- ğŸ“ ask_dependencies     # ğŸ”— Second step: Identify function dependencies
|
|-- ğŸ“ similarity_retrieval  # ğŸ” Third step: Retrieve the most relevant APIs
|-- ğŸ“ function_list_buildup # ğŸ“œ Build a function list for further reference
|-- ğŸ“„ final_completion.py   # ğŸ—ï¸ Generate complete code using retrieved APIs
|
|-- ğŸš€ run_pipeline.py       # â–¶ï¸ Execute the full pipeline

ğŸ“ empirical_study # Code for empierical study mentioned in our paper
|-- ğŸ“API # code for API retrieval and context+API retrieval
|-- ğŸ“LLM # code for context retrieval and only LLM generation(without retrieval)
|-- ğŸ“SimCode # code for similarity retrieval and simCode+context retrieval
|-- ğŸ“SimCode+API # code for SimCode+API retrieval and context+SimCOde+API retrieval
```

## âš¡ QuickStart

### ğŸ“ 1. Prepare the Input Files

ğŸ“Œ Ensure you have input.jsonl and the input repository structured correctly in the input folder.
If you want to use RepoExec, CoderEval or ExecRepobench, you can follow the 

### ğŸ“¦ 2. Install Dependencies

Run the following command to install all required packages:

```sh
pip install contexttimer flask transformers_stream_generator colorama accelerate python-Levenshtein tqdm sentence_transformers flash_attn
```

### ğŸ“œ 3. Prepare repository function extraction

In line 55 of repo_funcs_summary/repo_funcs_extraction.py, change the repo_path string into your repository path.

```python
repo_path = 'input/string_utils' # change your path here
```

### â–¶ï¸ 4. Run the Pipeline

```sh
python run_pipeline.py
```

## ğŸ“Š Datasets

In our paper, we evaluate AllianceCoder using three function-level Python code generation benchmarks: **RepoExec**, **CoderEval**, and **ExecRepoBench**. Each dataset presents unique challenges relevant to real-world repository-level code completion.

---

### ğŸ§ª **1. RepoExec**

> **Focus:** Repository-level code completion with complex contextual dependencies.

- âœ… Evaluates the ability to generate **functionally correct and executable code** while utilizing **cross-file contexts**.
- ğŸ§© Each task includes developer-specified **code dependencies** and **comprehensive test cases**.
- ğŸ“š Ideal for assessing how well models understand and integrate repository-wide knowledge.

- ğŸ“¦ **Dataset**: [Hugging Face â€“ RepoExec](https://huggingface.co/datasets/Fsoft-AIC/RepoExec)  
- ğŸ“„ **Paper**: [arXiv:2406.11927v2](https://arxiv.org/abs/2406.11927v2)

---

### ğŸ§ª **2. CoderEval**

> **Focus:** Pragmatic function-level code generation across real-world tasks.

- ğŸ› ï¸ Contains **230 Python** and **230 Java** tasks sampled from **open-source repositories**.
- ğŸ§¾ Each task provides:
  - A **function signature**
  - A **natural language description**
  - A **reference solution**
  - **Unit tests** for functional verification

- ğŸ“¦ **Dataset**: [GitHub â€“ CoderEval](https://github.com/CoderEval/CoderEval)  
- ğŸ“„ **Paper**: [ACM DL](https://dl.acm.org/doi/10.1145/3597503.3623316)

---

### ğŸ§ª **3. ExecRepoBench**

> **Focus:** Code completion benchmark with AST-guided multi-level masking.

- ğŸ§  Based on **1,200 samples** from real Python repositories.
- ğŸ§© Simulates **statement**, **expression**, and **function-level** masking guided by **abstract syntax trees (ASTs)**.
- âš™ï¸ Originally designed for **block completion**; we adapt it for **function-level generation**:
  - Reviewed the **167 test samples**
  - Selected and transformed those suitable into full **function-level generation tasks**

- ğŸ“¦ **Codebase**: [ExecRepoBench](https://execrepobench.github.io)  
- ğŸ“„ **Paper**: [arXiv:2412.11990](https://arxiv.org/abs/2412.11990)  
- ğŸ“ **Modified Data File**: `execrepobench_data.jsonl` (included in this repo)

---